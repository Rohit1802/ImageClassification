{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a772fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a704c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Admission_Predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "044bada1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>330</td>\n",
       "      <td>115</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>321</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>308</td>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>302</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>323</td>\n",
       "      <td>108</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "5           6        330          115                  5  4.5   3.0  9.34   \n",
       "6           7        321          109                  3  3.0   4.0  8.20   \n",
       "7           8        308          101                  2  3.0   4.0  7.90   \n",
       "8           9        302          102                  1  2.0   1.5  8.00   \n",
       "9          10        323          108                  3  3.5   3.0  8.60   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  \n",
       "5         1              0.90  \n",
       "6         1              0.75  \n",
       "7         0              0.68  \n",
       "8         0              0.50  \n",
       "9         0              0.45  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eff6e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db576c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         400 non-null    int64  \n",
      " 1   GRE Score          400 non-null    int64  \n",
      " 2   TOEFL Score        400 non-null    int64  \n",
      " 3   University Rating  400 non-null    int64  \n",
      " 4   SOP                400 non-null    float64\n",
      " 5   LOR                400 non-null    float64\n",
      " 6   CGPA               400 non-null    float64\n",
      " 7   Research           400 non-null    int64  \n",
      " 8   Chance of Admit    400 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 28.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86745c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f323fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Serial No.'] ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4967b46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0        337          118                  4  4.5   4.5  9.65         1   \n",
       "1        324          107                  4  4.0   4.5  8.87         1   \n",
       "2        316          104                  3  3.0   3.5  8.00         1   \n",
       "3        322          110                  3  3.5   2.5  8.67         1   \n",
       "4        314          103                  2  2.0   3.0  8.21         0   \n",
       "\n",
       "   Chance of Admit   \n",
       "0              0.92  \n",
       "1              0.76  \n",
       "2              0.72  \n",
       "3              0.80  \n",
       "4              0.65  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b49c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f791d44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "0          337          118                  4  4.5   4.5  9.65         1\n",
       "1          324          107                  4  4.0   4.5  8.87         1\n",
       "2          316          104                  3  3.0   3.5  8.00         1\n",
       "3          322          110                  3  3.5   2.5  8.67         1\n",
       "4          314          103                  2  2.0   3.0  8.21         0\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "395        324          110                  3  3.5   3.5  9.04         1\n",
       "396        325          107                  3  3.0   3.5  9.11         1\n",
       "397        330          116                  4  5.0   4.5  9.45         1\n",
       "398        312          103                  3  3.5   4.0  8.78         0\n",
       "399        333          117                  4  5.0   4.0  9.66         1\n",
       "\n",
       "[400 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c2e2bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.92\n",
       "1      0.76\n",
       "2      0.72\n",
       "3      0.80\n",
       "4      0.65\n",
       "       ... \n",
       "395    0.82\n",
       "396    0.84\n",
       "397    0.91\n",
       "398    0.67\n",
       "399    0.95\n",
       "Name: Chance of Admit , Length: 400, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d858c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfafda4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>301</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>334</td>\n",
       "      <td>119</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>305</td>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>307</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>318</td>\n",
       "      <td>106</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>307</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>321</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>326</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>300</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "93         301           97                  2  3.0   3.0  7.88         1\n",
       "23         334          119                  5  5.0   4.5  9.70         1\n",
       "299        305          112                  3  3.0   3.5  8.65         0\n",
       "13         307          109                  3  4.0   3.0  8.00         1\n",
       "90         318          106                  2  4.0   4.0  7.92         1\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "255        307          110                  4  4.0   4.5  8.37         0\n",
       "72         321          111                  5  5.0   5.0  9.45         1\n",
       "396        325          107                  3  3.0   3.5  9.11         1\n",
       "235        326          111                  5  4.5   4.0  9.23         1\n",
       "37         300          105                  1  1.0   2.0  7.80         0\n",
       "\n",
       "[320 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a34e7b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2b00a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22      , 0.17857143, 0.25      , ..., 0.42857143, 0.25      ,\n",
       "        1.        ],\n",
       "       [0.88      , 0.96428571, 1.        , ..., 0.85714286, 0.91911765,\n",
       "        1.        ],\n",
       "       [0.3       , 0.71428571, 0.5       , ..., 0.57142857, 0.53308824,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.7       , 0.53571429, 0.5       , ..., 0.57142857, 0.70220588,\n",
       "        1.        ],\n",
       "       [0.72      , 0.67857143, 1.        , ..., 0.71428571, 0.74632353,\n",
       "        1.        ],\n",
       "       [0.2       , 0.46428571, 0.        , ..., 0.14285714, 0.22058824,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e880fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d99cdb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(7 ,activation='relu', input_dim=7))\n",
    "model.add(Dense(7 ,activation='relu'))\n",
    "model.add(Dense(7 ,activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35660c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 7)                 56        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 56        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 56        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 176\n",
      "Trainable params: 176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8360019",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15e1c229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 1s 55ms/step - loss: 0.7165 - val_loss: 0.6939\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6114 - val_loss: 0.6132\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5491 - val_loss: 0.5847\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5231 - val_loss: 0.5586\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5019 - val_loss: 0.5360\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4860 - val_loss: 0.5209\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4737 - val_loss: 0.5086\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4623 - val_loss: 0.4972\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4514 - val_loss: 0.4864\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4411 - val_loss: 0.4759\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4310 - val_loss: 0.4658\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4211 - val_loss: 0.4559\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4116 - val_loss: 0.4462\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4023 - val_loss: 0.4366\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3931 - val_loss: 0.4272\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3843 - val_loss: 0.4180\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3755 - val_loss: 0.4089\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3670 - val_loss: 0.3999\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3586 - val_loss: 0.3910\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3503 - val_loss: 0.3824\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3422 - val_loss: 0.3740\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3343 - val_loss: 0.3656\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3265 - val_loss: 0.3574\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3189 - val_loss: 0.3493\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3114 - val_loss: 0.3414\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3040 - val_loss: 0.3337\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2968 - val_loss: 0.3260\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2897 - val_loss: 0.3186\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2829 - val_loss: 0.3112\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2761 - val_loss: 0.3040\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2694 - val_loss: 0.2968\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2628 - val_loss: 0.2899\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2564 - val_loss: 0.2832\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2502 - val_loss: 0.2765\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2441 - val_loss: 0.2699\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2381 - val_loss: 0.2635\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2321 - val_loss: 0.2573\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2264 - val_loss: 0.2511\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2208 - val_loss: 0.2451\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2153 - val_loss: 0.2392\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2099 - val_loss: 0.2334\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2046 - val_loss: 0.2277\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1995 - val_loss: 0.2222\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1944 - val_loss: 0.2168\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1895 - val_loss: 0.2114\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1846 - val_loss: 0.2062\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1799 - val_loss: 0.2012\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1753 - val_loss: 0.1962\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1708 - val_loss: 0.1913\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1664 - val_loss: 0.1866\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1621 - val_loss: 0.1818\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1578 - val_loss: 0.1773\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1537 - val_loss: 0.1729\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1498 - val_loss: 0.1685\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1459 - val_loss: 0.1643\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1421 - val_loss: 0.1601\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1383 - val_loss: 0.1561\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1347 - val_loss: 0.1521\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1312 - val_loss: 0.1482\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1277 - val_loss: 0.1444\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1243 - val_loss: 0.1407\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1211 - val_loss: 0.1371\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1179 - val_loss: 0.1336\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1147 - val_loss: 0.1302\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1117 - val_loss: 0.1268\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1088 - val_loss: 0.1235\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1059 - val_loss: 0.1203\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1031 - val_loss: 0.1172\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1004 - val_loss: 0.1142\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0977 - val_loss: 0.1112\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0952 - val_loss: 0.1083\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0927 - val_loss: 0.1055\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0902 - val_loss: 0.1028\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0878 - val_loss: 0.1001\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0856 - val_loss: 0.0975\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0833 - val_loss: 0.0950\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0811 - val_loss: 0.0925\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0790 - val_loss: 0.0901\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0770 - val_loss: 0.0878\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0750 - val_loss: 0.0855\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0731 - val_loss: 0.0833\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0712 - val_loss: 0.0812\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0693 - val_loss: 0.0791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0676 - val_loss: 0.0771\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0659 - val_loss: 0.0751\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0642 - val_loss: 0.0732\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0626 - val_loss: 0.0713\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0610 - val_loss: 0.0695\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0595 - val_loss: 0.0677\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0581 - val_loss: 0.0660\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0566 - val_loss: 0.0644\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0553 - val_loss: 0.0627\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0540 - val_loss: 0.0612\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0527 - val_loss: 0.0596\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0514 - val_loss: 0.0582\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0502 - val_loss: 0.0567\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0491 - val_loss: 0.0553\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0480 - val_loss: 0.0540\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0469 - val_loss: 0.0527\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0458 - val_loss: 0.0514\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0448 - val_loss: 0.0502\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0438 - val_loss: 0.0490\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0429 - val_loss: 0.0478\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0420 - val_loss: 0.0467\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0411 - val_loss: 0.0456\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0403 - val_loss: 0.0446\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0394 - val_loss: 0.0436\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0386 - val_loss: 0.0426\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0379 - val_loss: 0.0416\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0371 - val_loss: 0.0407\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0364 - val_loss: 0.0398\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0357 - val_loss: 0.0390\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0351 - val_loss: 0.0381\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0345 - val_loss: 0.0373\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0339 - val_loss: 0.0365\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0333 - val_loss: 0.0358\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0327 - val_loss: 0.0350\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0322 - val_loss: 0.0343\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0317 - val_loss: 0.0336\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0312 - val_loss: 0.0330\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0307 - val_loss: 0.0323\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0302 - val_loss: 0.0317\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0298 - val_loss: 0.0311\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0294 - val_loss: 0.0305\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0290 - val_loss: 0.0300\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0286 - val_loss: 0.0295\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0282 - val_loss: 0.0289\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0278 - val_loss: 0.0284\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0275 - val_loss: 0.0279\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0272 - val_loss: 0.0275\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0269 - val_loss: 0.0270\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0266 - val_loss: 0.0266\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0263 - val_loss: 0.0262\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0260 - val_loss: 0.0258\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0257 - val_loss: 0.0254\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0255 - val_loss: 0.0250\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0252 - val_loss: 0.0246\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0250 - val_loss: 0.0243\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0248 - val_loss: 0.0239\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0246 - val_loss: 0.0236\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0244 - val_loss: 0.0233\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0242 - val_loss: 0.0230\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0240 - val_loss: 0.0227\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0239 - val_loss: 0.0224\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0237 - val_loss: 0.0222\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0235 - val_loss: 0.0219\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0234 - val_loss: 0.0217\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0232 - val_loss: 0.0214\n",
      "Epoch 149/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0231 - val_loss: 0.0212\n",
      "Epoch 150/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0230 - val_loss: 0.0210\n",
      "Epoch 151/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0228 - val_loss: 0.0207\n",
      "Epoch 152/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0227 - val_loss: 0.0205\n",
      "Epoch 153/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0226 - val_loss: 0.0203\n",
      "Epoch 154/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0225 - val_loss: 0.0201\n",
      "Epoch 155/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0224 - val_loss: 0.0198\n",
      "Epoch 156/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0222 - val_loss: 0.0195\n",
      "Epoch 157/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0220 - val_loss: 0.0192\n",
      "Epoch 158/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0219 - val_loss: 0.0189\n",
      "Epoch 159/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0216 - val_loss: 0.0186\n",
      "Epoch 160/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0211 - val_loss: 0.0188\n",
      "Epoch 161/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0206 - val_loss: 0.0185\n",
      "Epoch 162/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0197 - val_loss: 0.0177\n",
      "Epoch 163/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0192 - val_loss: 0.0174\n",
      "Epoch 164/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0186 - val_loss: 0.0171\n",
      "Epoch 165/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0181 - val_loss: 0.0170\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0176 - val_loss: 0.0168\n",
      "Epoch 167/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0172 - val_loss: 0.0166\n",
      "Epoch 168/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0168 - val_loss: 0.0162\n",
      "Epoch 169/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0165 - val_loss: 0.0160\n",
      "Epoch 170/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0161 - val_loss: 0.0159\n",
      "Epoch 171/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0159 - val_loss: 0.0156\n",
      "Epoch 172/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0156 - val_loss: 0.0152\n",
      "Epoch 173/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0153 - val_loss: 0.0152\n",
      "Epoch 174/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0150 - val_loss: 0.0147\n",
      "Epoch 175/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0149 - val_loss: 0.0145\n",
      "Epoch 176/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0146 - val_loss: 0.0146\n",
      "Epoch 177/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0141\n",
      "Epoch 178/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0141 - val_loss: 0.0138\n",
      "Epoch 179/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0138\n",
      "Epoch 180/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0137 - val_loss: 0.0133\n",
      "Epoch 181/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0135 - val_loss: 0.0132\n",
      "Epoch 182/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0133 - val_loss: 0.0132\n",
      "Epoch 183/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0131 - val_loss: 0.0128\n",
      "Epoch 184/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0130 - val_loss: 0.0126\n",
      "Epoch 185/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0127 - val_loss: 0.0127\n",
      "Epoch 186/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0126 - val_loss: 0.0123\n",
      "Epoch 187/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0124 - val_loss: 0.0121\n",
      "Epoch 188/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0123 - val_loss: 0.0120\n",
      "Epoch 189/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0121 - val_loss: 0.0117\n",
      "Epoch 190/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0120 - val_loss: 0.0117\n",
      "Epoch 191/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0118 - val_loss: 0.0115\n",
      "Epoch 192/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0117 - val_loss: 0.0112\n",
      "Epoch 193/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0115 - val_loss: 0.0113\n",
      "Epoch 194/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 195/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0114 - val_loss: 0.0108\n",
      "Epoch 196/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 197/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0111 - val_loss: 0.0105\n",
      "Epoch 198/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 199/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 200/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0107 - val_loss: 0.0103\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=200, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0053b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "017087f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4866796465576567"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaa4a8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x222750f60b0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAr9ElEQVR4nO3deXxU1f3/8ddnZjKTfSEECGvYJQgCRhBFUXHBDbRWRa1V61pr1WoX/Fqt1Vq11v60aKu4VOuGaG1FQXFfkDXIImGRsCcsCYTsy2Rmzu+POyEjJhAgmZuZfJ6Pxzwyc87N3A83wzs3dzlHjDEopZSKfA67C1BKKdU6NNCVUipKaKArpVSU0EBXSqkooYGulFJRwmXXijt37myysrLsWr1SSkWkpUuX7jbGZDTVZ1ugZ2VlkZuba9fqlVIqIonIlub69JCLUkpFCQ10pZSKEhroSikVJTTQlVIqSmigK6VUlNBAV0qpKKGBrpRSUSLiAn3J5hIenbsWf0CH/VVKqVARF+jLt5by1GcbqPL67C5FKaXalYgL9ASPdXNrVZ0GulJKhWpRoIvIRBFZJyL5IjK1if7/JyLLg4/vRKS01SsNSozVQFdKqaYcdCwXEXECTwFnAAXAEhGZZYxZ3bCMMeZXIcv/EhjZBrUCkOhxAlBRq4GulFKhWrKHPhrIN8ZsNMZ4gRnA5AMsfxnwemsU15QEd8Meur+tVqGUUhGpJYHeA9gW8rog2PYDItIH6At82kz/DSKSKyK5xcXFh1orACmUM0w2Ullbf1jfr5RS0aq1T4pOAd4yxjS5+2yMmW6MyTHG5GRkNDmc70F1zZ/Ju57fU1NdcSR1KqVU1GlJoBcCvUJe9wy2NWUKbXi4BcCV0AkAX9XetlyNUkpFnJYE+hJgoIj0FRE3VmjP2n8hETkKSAMWtG6J3+dJSgfAV1XSlqtRSqmIc9BAN8b4gFuAucAaYKYxJk9E7heRSSGLTgFmGGPa9BbOmOAeuqkpbcvVKKVUxGnRFHTGmDnAnP3a7t3v9X2tV1bzJC7VelKjh1yUUipUxN0pSlwaAFJXam8dSinVzkRgoKcC4Kors7cOpZRqZyIv0N1J+HEQ49VAV0qpUJEX6A4H1Y4E3PXldleilFLtSuQFOlDjTCbWpzcWKaVUqIgM9FpXEnF+DXSllAoVkYHudSWTENBAV0qpUBEZ6D5PKkmmkja+h0kppSJKRAa6351CslRRU69D6CqlVIOIDPRAbAopVFFZo0PoKqVUg4gMdIlLwyUBqipL7S5FKaXajYgNdIC6ch1xUSmlGkRkoDsTUgGordxjbyFKKdWORGSguxOCY6JX6h66Uko1iMxAT7LGRA9U6xC6SinVICIDPTbZ2kPXQFdKqUYRGejxKVag6yQXSinVKCIDPS4+Ga9xIjomulJK7RORgS4OB8XSifjq7XaXopRS7UaLAl1EJorIOhHJF5GpzSxziYisFpE8EXmtdcv8oY3OvmRUrWvr1SilVMQ46CTRIuIEngLOAAqAJSIyyxizOmSZgcBdwInGmL0i0qWtCm5QGDuQE6uWgLcK3AltvTqllGr3WrKHPhrIN8ZsNMZ4gRnA5P2WuR54yhizF8AYU9S6Zf7QnsSjcGBgV15br0oppSJCSwK9B7At5HVBsC3UIGCQiHwtIgtFZGJTbyQiN4hIrojkFhcXH17FQRVp2daTHSuO6H2UUipatNZJURcwEDgFuAx4VkRS91/IGDPdGJNjjMnJyMg4ohW603pSYhIJaKArpRTQskAvBHqFvO4ZbAtVAMwyxtQbYzYB32EFfJtJT4olL5BFYLsGulJKQcsCfQkwUET6iogbmALM2m+Z/2HtnSMinbEOwWxsvTJ/qHOihzzTF0fxWvB523JVSikVEQ4a6MYYH3ALMBdYA8w0xuSJyP0iMim42Fxgj4isBj4DfmOMadOhENMT3awKZOEIeKF4TVuuSimlIsJBL1sEMMbMAebs13ZvyHMD3BF8hEXnRDfLTX/rReFSyDwmXKtWSql2KSLvFAXrkEuByaA2JtUKdKWU6uAiNtCTY2NwORwUJgyBwmV2l6OUUraL2EB3OIT0RDebYgZbx9DrKu0uSSmlbBWxgQ6QnuAhTwaACegNRkqpDi+iA71zkoel/n7Wi+3f2FuMUkrZLLIDPcHNhqo4SOkN2xbbXY5SStkqsgM9ycOeqjpMn+Nh6wIwxu6SlFLKNhEd6OkJbmrrA9T1GAtVxbBng90lKaWUbSI60HumxQOwJWGE1bDla/uKUUopm0V0oA/vmQLA4opOkJABW+bbXJFSStknogO9Z1oc6QluVhaUQZ8TNNCVUh1aRAe6iHBMr1RWFJRCnxOhbCvs3Wx3WUopZYuIDnSAY3qmsr6okqqs062GVf+xtyCllLJJ5Ad6rxSMgZWVqdB7LKx4Qy9fVEp1SJEf6D1TAazDLsMvgd3rYMdyO0tSSilbRHygpyW4yUqPZ+mWvTD0QnC6YcUMu8tSSqmwi/hABzi+XzqLNu7B70mFIefDslehtszuspRSKqyiItDH9k+nvNbH6u3lcMIvwVsBS1+0uyyllAqr6Aj0fukAzN+wG7qPhL7jYeE/wVdnc2VKKRU+LQp0EZkoIutEJF9EpjbRf7WIFIvI8uDjutYvtXldkmMZ0CWR+RuC81KfeBtU7IBv3wxnGUopZauDBrqIOIGngLOBbOAyEcluYtE3jDEjgo/nWrnOgzqhfzpLNpdQ7w9A/9Og6zCYPw0CgXCXopRStmjJHvpoIN8Ys9EY4wVmAJPbtqxDd+KAzlR7/SzaWAIicOKtULwW1n9od2lKKRUWLQn0HsC2kNcFwbb9XSQiK0XkLRHp1dQbicgNIpIrIrnFxcWHUW7zxg/KIMHt5L2V262GoRdCSi/46jG90Ugp1SG01knRd4EsY8xw4CPgpaYWMsZMN8bkGGNyMjIyWmnVltgYJ2cO7cb7q3bi9QXAGQPjfgUFiyH/k1Zdl1JKtUctCfRCIHSPu2ewbR9jzB5jTMMlJc8Bx7ZOeYfm/GMyKaup56v1wb3/kVdCam/49AHdS1dKRb2WBPoSYKCI9BURNzAFmBW6gIhkhrycBKxpvRJbbtyADFLjY3h7WfD3jcsN46daQwGsnW1HSUopFTYHDXRjjA+4BZiLFdQzjTF5InK/iEwKLnariOSJyArgVuDqtir4QNwuBz8e1ZMPVu2kYG+11Tj8UkgfAJ89qFe8KKWiWouOoRtj5hhjBhlj+htjHgy23WuMmRV8fpcxZqgx5hhjzKnGmLVtWfSB/GxcXwR4ft4mq8HpglPugqLVkPe2XWUppVSbi4o7RUN1T41j0ojuzFi8jb1VXqtx6I+gy1BrL91fb2+BSinVRqIu0AFuOLkfNfV+Xlm4xWpwOGDCvVCyEXL/ZW9xSinVRqIy0I/qlsypgzN4cf5mauv9VuOgsyDrJPjiYR2JUSkVlaIy0AFuHN+fPVVe3lpaYDWIwJkPQPUemPe4rbUppVRbiNpAH9O3EyN6pfL0FxusG43AGolx2MWw8B9QVmBvgUop1cqiNtBFhNtPH0jB3hpm5oaMXHDaPWAC8OmD9hWnlFJtIGoDHazxXXL6pDHt0/WNx9LT+sCYG2HF67DzW3sLVEqpVhTVgS4i3HnmYHaV1/Hqoq2NHSfdCbEp8OE99hWnlFKtLKoDHazp6U4ckM4/P8+nqs5nNcalwfjfwsbPYP3H9haolFKtJOoDHeDOMwezu9LLi/M3NzYedx106gdz79KbjZRSUaFDBPqo3mlMOKoLz3yxofHuUZcHzvoz7P4OFj9rb4FKKdUKOkSgA/zu7KOorPPx90/XNzYOmgj9J8DnD0PVbvuKU0qpVtBhAn1Q1yQuPa43Ly/YwqbdVVajCEx8COqrrDHTlVIqgnWYQAf41RkD8bgcPPJ+yGCQGYNh9A2w9CXYscK+4pRS6gh1qEDvkhTLTeP780HeTpZsLmnsGP87iE+H93+nMxsppSJWhwp0gOtO6kfXZA9/mr0G0xDecakw4R7YukDHTFdKRawOF+hxbie/PnMwK7aV8u7KHY0dI6+EbsOtm428VfYVqJRSh6nDBTrAj0b1ZEhmMo+8v7ZxSACHE855FMoL4Yu/2FugUkodhg4Z6E6HcPc5QygsreGFrzc1dvQ+Hkb8BBY8CUW2zaKnlFKHpUWBLiITRWSdiOSLyNQDLHeRiBgRyWm9EtvGuIGdOTO7K9M+yWd7aU1jxxl/BHcizL5TT5AqpSLKQQNdRJzAU8DZQDZwmYhkN7FcEnAbsKi1i2wr95yXjcHwp9mrGxsTOsPp98GWebBypm21KaXUoWrJHvpoIN8Ys9EY4wVmAJObWO4B4BGgthXra1O9OsXzi1MGMOfbnXy1vrixY9RV0ONY+PD3UFNqW31KKXUoWhLoPYCQGSIoCLbtIyKjgF7GmNkHeiMRuUFEckUkt7i4+ECLhs31J/cjKz2eP7yTR52v4QSpA879G1Tvhs90IgylVGQ44pOiIuIA/gbcebBljTHTjTE5xpicjIyMI111q4iNcXLfpKFs3F3F8/NCTpB2HwHHXQ9LnoPty2yrTymlWqolgV4I9Ap53TPY1iAJOBr4XEQ2A8cDsyLhxGiDUwZ34ayh1gnSwtATpKfdDQkZMOtW8PvsK1AppVqgJYG+BBgoIn1FxA1MAWY1dBpjyowxnY0xWcaYLGAhMMkYk9smFbeRhhOkD7wbcoI0NsW6Nn3nSlgwzb7ilFKqBQ4a6MYYH3ALMBdYA8w0xuSJyP0iMqmtCwyXnmnx/PK0gXyQt5NP1uxq7MieDEMmwWcPwe71zb+BUkrZTIxN11rn5OSY3Nz2tRPv9QU4f9o8ymvr+eiO8SR6XFZHxS546jjokg1Xz7FOmiqllA1EZKkxpslD2ppMIdwuBw9dNIyd5bX8de66xo6krnDWQ9bgXbnP21egUkodgAb6fkb1TuOqsVm8tGAzS7fsbewYcTn0OxU+vg9Kt9pWn1JKNUcDvQm/Pmswmcmx3PX2Sry+gNUoAuc/YQ0H8N6vdFgApVS7o4HehESPiwcuOJrvdlXyzBcbGjvS+sCEeyH/Y1j5hn0FKqVUEzTQmzFhSFfOHZ7JtE/zyS+qbOwYfT30GgMfTIXKIvsKVEqp/WigH8B95w8lzu3kt2+twB8IHmJxOGHSNGsSjNl36KEXpVS7oYF+ABlJHu6blM03W0t57quNIR2D4ZS7YM27euhFKdVuaKAfxAUjenBmdlce++g71u+qaOw48TbodTzM+Q2Ubmv+DZRSKkw00A9CRHjwwmEkuJ3c+eYKfP7gVS8OJ1z4Twj44Z2bIRCwt1ClVIengd4CGUke/nTBMFYWlPHPz0OueunUDyb+GTZ9CYufsa9ApZRCA73Fzh2eyXnDM/n7p+tZvb28sWPUVTBoonXDUfG6Zr9fKaXamgb6IXhg8tGkxLm5Y+by/W44+jvExMPbN4C/3t4ilVIdlgb6IUhLcPPwj4axdmcFj32031gv5z8BO5brDEdKKdtooB+i07O7cvmY3kz/ciPz83c3dmRPsg6/zHscNnxmW31KqY5LA/0w3HNuNn07J/CrmcvZW+Vt7Jj4sHWN+n9v1LtIlVJhp4F+GOLcTv4+ZSQlVV6mvr2SfWPKu+Phxy9AbRn89ya9lFEpFVYa6Ifp6B4p/OaswczN28UbS0JuLOo6FM76M2z4BBY8aV+BSqkORwP9CFw3rh8nDkjnj++uZkNxyABeOT+zpq375I9QuNS+ApVSHYoG+hFwOITHLh6BJ8bBra8vo7beb3WIwKS/Q1ImvPUzqCm1tU6lVMfQokAXkYkisk5E8kVkahP9N4nItyKyXETmiUh265faPnVLieWxi48hb3s5D7y3urEjLs06nl5WAP+7WUdlVEq1uYMGuog4gaeAs4Fs4LImAvs1Y8wwY8wI4C/A31q70PZswpCu3Di+H68u2so7ywsbO3qNhjP/BOtmw9dP2FegUqpDaMke+mgg3xiz0RjjBWYAk0MXMMaE3AtPAtDhdkd/feZgjstK4663v/3+hBhjboKhF1rH0zfPs69ApVTUa0mg9wBCx4ctCLZ9j4j8QkQ2YO2h39rUG4nIDSKSKyK5xcXFh1NvuxXjdDDtslHExji5+dWl1HhDj6dPg0794c1roGKnvYUqpaJWq50UNcY8ZYzpD/wO+H0zy0w3xuQYY3IyMjJaa9XtRreUWB6/dATriyq5551VjR2eJLj0ZfBWWidJ/T77ilRKRa2WBHoh0Cvkdc9gW3NmABccQU0R7eRBGfzytIG8tbSA1xZtbezoMsQa72XL1/DRvfYVqJSKWi0J9CXAQBHpKyJuYAowK3QBERkY8vJcYH3rlRh5bpswkPGDMvjDrFUs2VzS2DH8EuuY+sKnYNmr9hWolIpKBw10Y4wPuAWYC6wBZhpj8kTkfhGZFFzsFhHJE5HlwB3AVW1VcCRwOoS/TxlJz7R4fv7KUraX1jR2nvkg9DsF3rsdti22q0SlVBQSY9P10Tk5OSY3N9eWdYdLflEFFzw1n6zO8bx54wnEuZ1WR3UJPHsaeKvghs8h5QfnmJVSqkkistQYk9NUn94p2oYGdEni8UtHkLe9nN/9J2QQr/hOcNkMqK+BGZeDt9reQpVSUUEDvY2dnt2VX585mFkrtvPMlxsbO7ocBRc9BztWwDu/0DtJlVJHTAM9DG4+pT/nDsvkkQ/W8tnakHHSB0+E0/8AeW/Dp3+yr0ClVFTQQA8DEeHRi4dzVLdkbp2x7PsjM554O4z6KXz1V1j6km01KqUinwZ6mMS7XTz702NxOx1c868l7K6sszpE4Ny/Qf8J8N6vYP3H9haqlIpYGuhh1DMtnueuyqGoopZrX8ptHB7AGQOXvARds+HNq2DHSnsLVUpFJA30MBvZO40npoxkZUEpt81Yhj8QPBnqSYLL34TYFHj1YijdduA3Ukqp/Wig2+Csod34w3nZfLh6Fw+8t7rxcsbkTLjiTaivhld/bF2vrpRSLaSBbpOrT+zLdeP68uL8zTw/b1NjR9ehMOU1KNlkhXpdZfNvopRSITTQbfR/5wzh7KO78afZa/jfspDxzvqeBBf/C7YvhzeuAF+dbTUqpSKHBrqNHA7h/106grH90rnzzRV8unZXY+dR58LkJ2Hj5/D29RDw21anUioyaKDbLDbGyfSfHkt2ZjI/f+UbFm8KOW4+4nJrMK/V71iXNOrdpEqpA9BAbweSYmN48Zrj6JkWx7UvLmFVYVlj5wm3wEl3wjcvwdz/01BXSjVLA72dSE/08PK1Y0iOi+GqFxazMfRu0tPugTE/h4X/gI/u0VBXSjVJA70d6Z4ax8vXjgbg8mcXsXl3ldUhAhMfguOuh/nT4OP7NNSVUj+ggd7O9MtI5JXrxlDn83PZswvZuic4tK4InPMo5PwMvn7cGsxLQ10pFUIDvR0akpnMq9cdT029FerbSkJD/bHGwbw+f0hDXSm1jwZ6O5XdPZlXrxtDZZ2PKdNDQt3hgPOegJE/gS8e0WPqSql9NNDbsaHdU/aF+vf21B0OOH9a4zH12XdAIGBvsUop27Uo0EVkooisE5F8EZnaRP8dIrJaRFaKyCci0qf1S+2Yju6RwivXjqGi1sfFTy8gvyh49YvDYR1TP/F2yH0B/vdz8PtsrVUpZa+DBrqIOIGngLOBbOAyEcneb7FlQI4xZjjwFvCX1i60IxvWM4U3bjweX8Bw6TMLyNsevE5dBE6/D077PaycAW9dAz6vrbUqpezTkj300UC+MWajMcYLzAAmhy5gjPnMGNMw0/FCoGfrlqmO6pbMzBuPx+NycNn0hSzdstfqEIGTfwNnPQRrZlkDetWW21usUsoWLQn0HkDo4NwFwbbmXAu831SHiNwgIrkikltcXNzyKhVgXdI486axdEpwc+Xzi5ifv7uxc+zNcMHTsOVr+Nc5UL7DvkKVUrZo1ZOiIvITIAd4tKl+Y8x0Y0yOMSYnIyOjNVfdYfRMi2fmTWPplRbP1f9awqwV2xs7R1wGl78BJRvh+TOgeJ19hSqlwq4lgV4I9Ap53TPY9j0icjpwNzDJGKPjvbahLkmxzLxxLCN6p3Lr68uY/uWGxkkyBpwO18y2htx9/kzYutDeYpVSYdOSQF8CDBSRviLiBqYAs0IXEJGRwDNYYV7U+mWq/aXEx/Dvn43m3OGZ/HnOWv747urG6ey6j4RrP4T4dHhpEqx8095ilVJhcdBAN8b4gFuAucAaYKYxJk9E7heRScHFHgUSgTdFZLmIzGrm7VQrio1xMm3KyH0zH9386lJq64PjpnfqC9d+BD1z4O3r4JMH9Fp1paKcGJvuMszJyTG5ubm2rDsaPT9vE3+avZphPVKYfmUO3VJirQ6f17rxaNnLcNR58KPp4E6wt1il1GETkaXGmJym+vRO0Shx7bi+TL8yhw1FlUx6ch7Lt5VaHS43TJpmXda4bg68cBaUFdhaq1KqbWigR5Ezsrvyn5tPwO1ycMkzCxrnKRWxLmu8fCaUbIZnToaNX9haq1Kq9WmgR5mjuiUz65ZxjOyVyu1vLOfh99fi8wePnQ88A67/FOI7w8sXwLzHdWAvpaKIBnoU6pTg5uVrx3DFmN48/cUGfvL8Iooqaq3OjEFWqA+ZBB//AWZeqXeWKhUlNNCjlNvl4MELh/HXi49h+bZSznliHvM3BO8s9STCxS9aE1CvnQPPngY7v7W1XqXUkdNAj3I/PrYn7/xiHClxLn7y3CKe/HQ9gYCxjqufcAtcNQvqKqxQX/APvbRRqQimgd4BDO6WxKxbxnHe8O789cPvuPrFJZRUBUdlzBoHP59v3WE69y547WKo1HvDlIpEGugdRILHxRNTRvDghUezcMMeznniq8ZDMAnpMOU1OPcx2DwP/jEWvvvQ3oKVUodMA70DERGuGNOHt28+gXi3kyueW8Sf56yhzue3DsEcdx3c8DkkdrX21Of8Fupr7C5bKdVCGugd0NE9Unjv1nFcPro307/cyOQnv2bdzgqrs8sQ6yqYMTfB4mfg6XE6wJdSEUIDvYOKd7t48MJhvHB1Drsr6zj/yXlM/3KDdc16TCyc/Qhc+T9r6IAXJsL7U8FbZXfZSqkD0EDv4E47qisf3H4y4wdl8Oc5a7nwH/Mbp7jrfyrcvMA6FLPon/DPE2DTV/YWrJRqlga6onOih+lXHstTl49iR1ktk578mofeX0ON129ds37uX+Hq2YDAS+fB2zfqlTBKtUMa6AqwTpieOzyTT+4Yz8XH9uSZLzYy8Ykv+bphmruGyxtP/g3kvQ3TcmDRdAj47S1cKbWPBrr6npT4GB6+aDivX388DhGueG4Rd7yxnKLyWnDHw2m/h58vgB4j4f3fwPRTYNsSu8tWSqHjoasDqK338+Sn+Uz/ciMxTuHmUwdw7bi+xMY4rUG98v4Lc/8PKnbAqJ/Cqb+HpK52l61UVDvQeOga6Oqgtuyp4sHZa/hw9S56dYrj7nOyOWtoV0TEGjbg84dh0dPg9MAJv7QenkS7y1YqKmmgq1bxdf5u7n93Net2VTC2Xzr3np/NkMxkq3PPBvjkflj9P0joAqfeBSN/Ck6XrTUrFW000FWr8fkDvL54K4999B1lNfVMOqY7vzp9EFmdg9PabVsCH90DWxdA50Ew4Q9w1LnWnahKqSN2xFPQichEEVknIvkiMrWJ/pNF5BsR8YnIj4+0YNV+uZwOrhybxee/PoWbxvfnw7xdTPjbF0z9z0oKS2ug13FwzfvW2DDGwBtXWCdO132gk2ko1cYOuocuIk7gO+AMoABYAlxmjFkdskwWkAz8GphljHnrYCvWPfToUFRRyz8+28Bri7YCcPmY3tx8an+6JMWC3wcrZ8AXf4HSLdB9JJxyFww8U/fYlTpMR3TIRUTGAvcZY84Kvr4LwBjzUBPLvgi8p4He8RSW1jDtk/W8ubSAGKdwSU4vrj+pH706xYO/Hla8Dl8+CqVbIXMEnHgbZE8Gh9Pu0pWKKEd6yKUHsC3kdUGw7XAKuUFEckUkt7i4+HDeQrVTPVLjePii4Xx8x3jOH96d1xdv5ZS/fs5tM5axeleNdVnjL7+BSdOsK2PeugamjYIlz+mIjkq1kpbsof8YmGiMuS74+kpgjDHmliaWfRHdQ1fAjrIaXpi3idcWbaXK62f8oAxuGt+f4/t1QkwA1s6Grx+HwqXWpNU518Cx10DKYe0rKNVhHOkeeiHQK+R1z2CbUs3KTInj7nOzmT91Ar85azB528u47NmFnP3EV7yyuICq/ufAdZ9YY8T0zIEv/wqPD4OZV1mTbOgJVKUOWUv20F1YJ0UnYAX5EuByY0xeE8u+iO6hqybU1vt5Z3kh/16whbzt5SR6XFw0qgdXju3DgC5JULIJcp+Hb16G2lLokg2jr4dhF4Mnye7ylWo3jvg6dBE5B3gccAIvGGMeFJH7gVxjzCwROQ74L5AG1AI7jTFDD/SeGugdkzGGZdtKeXnBFmav3IHXH2Bsv3QuOa4nZw3tRjxeWPUWLJ4OO7+FmHjr5OmIy6HPOHDo8EOqY9Mbi1S7tLuyjpm523ht0VYK9taQ4HZy9rBMfjSqB8dndcKxPReWvWKNGVNXDqm94ZjLYcRlkJZld/lK2UIDXbVrgYBhyeYS3v6mkNnf7qCyzkeP1DguHNmDSSO6MyjNaZ1EXf4KbPwCMNDreBh6IWRPguTudv8TlAobDXQVMWq8fj5cvZO3vynkq/XFBAwM7JLIOcMyOW94JgM9pdbNSnn/g12rAIHewXAfMgmSM23+FyjVtjTQVUQqqqhl7qqdvLdyB4s3l2AMDOiSyOlDunL6kC6MjC/GueYdK9yL8gCBXqNh0EQYfDZkHKV3pKqoo4GuIl5ReS0f5O3kg1U7WbypBF/AkBYfw6mDuzBhSFdO6VRCQv57sG427FhhfVNqHyvYB58NvU8Al9vef4RSrUADXUWV8tp6vvyumE/WFPHZuiJKq+txOYQRvVI5YUBnTsmsZ3jVQlz5c2HTF+CrhZgEaxq9fqdYjy5DdO9dRSQNdBW1/AHDN1v38tnaIr7esIdvC0oJGIiLcTK6bydO7hvPKTGrydq7EOfmL2BPvvWNiV2tYO97MvQeC536acCriKCBrjqMspp6Fm7cw/z83Xy9YQ/5RZUAuF0OhvdI4dTMOsa78hhQuZTYrV9CdXAS7MSu1snV3mOtR9ejdXIO1S5poKsOq6iilm+27GVp8LGqsByvPwBA306xTOxWzokx6xnsXUX6nm9wlAfHoYtJgMxjrCF/e4yyvqb11RublO000JUKqq33s6qwjKVb9pK7ZS8rC0rZVV63rz8ntYqzUzaT48wnq+47kkvXIP5aq9OTAt1HWOHebZg1PEH6AD3ZqsJKA12pAyiuqCNvexl528tZvb2cvO1lbN5TDYALH8PcOzg1qYBjYzYzwJdPRlU+DlNvfbPDBekDrZOsXbKDX4dYd7LqWO+qDWigK3WIymvrWbO9nPziSvKLKtlQXMWGokoKS2twU08/2cFRjm3kxO9kqKuQPv6tdPJu3/f9xhUHnQcgnfpDen8I/ZrQWU/AqsOmga5UK6n2+thYXMWG4ko2FFWSX1zJ1pJqtpXUUF9TwQApZLBjG4NlG4OcO+nv3Em3wC6cBPa9hy8mEV9qPxxpvYnp1AdJ7QUpvSClpzVeTVyaBr5q1oECXU/jK3UI4t0uju6RwtE9Un7QV15bz7aSagr21rCtpJpP99bwUkk1O0oqkLKtZNQX0Fd2kuXbSd/anfTYtZTu8iFx4v3e+3gdcVTFdqMuoTuBxEwcyd2ISelGbKcexKdlIkndrKty3PHh+merCKGBrlQrSY6NYWj3FIZ2/2HYg7V3X1Rex67yWooq6viuvJai8lqq9u7ClBYQU1VIUt1O0up3kVm/mx6VO+hatJrOlOGSwA/fT+Ipc3aiMiadanc6Xnca/tg0ArFpEN8JSUjHlZBGTGJn3EmdSUjuREJsDIkeFx6XA9G/AqKOBrpSYRLvdpHV2UVW54T9erK/9yoQMFTU+thb7WV7tZfVVbVU7y3CW7oDf8VOqCjCVbMLT81uEn17SKkrIbVmDSmmgmSqcEjTh1F9xkEpiRSaBCqIp0biqXYkUOOIp86RQK0zEa8rAa8zAV9MEv6YBPzuZAIxiRh3Ig5PAuJOJMbtwR3jxO104HY5cbsc1sPpwNPwPPi64XlDu8fZuLzTob9QWpsGulLtjMMhpMTHkBIfQxYN4Z8JHHPQ7/X7fFRU7KGmrJi6st14K3bjr9pDoKoEqkuQ2hKcdaUkeytJ91Xi9u/C7ask1l+Np74aBwc/p1ZvnFTjoYpYqk1syFcPFcRSZWKpxnpdbWKpxmO9bmg31ut4t5PMBCExKQVPaibJnbrSLTWerskeOid66JzkoXOiG49LrxZqKQ10paKI0+UiKa0rSWldD/2bAwHwVkJdhTWhSMPX2nLwVkF9NaauEkddJXG1lXjqKknzVmLqqsBbidRXId4yHPVVOHxVOH3V1oTgB1IdfOyCchNPmUmwfkHgYZPxkEcsXmccgZgEiEnAxCSAOwHxJODwJOKMTSQmLgl3XBJxHjeeGBcxCal4kjrhSUglwe0gPi4ed0zHiLqO8a9USh2cwwGxydaDHk0uIljzULZon9kYa2A0rxX41teQ5wi4PNbz8u0klmzEVVlKUk0F/ppKAt5KxFuO1O8kxl+Nu6aG2OraQ/5nBYxQRjwVkkC1JFDriKNO4vA64gg4YvA7PPidHgJON8bpwTg8BFweqzaXB5weHDGxiMuDw+XBEROD0+VB3HE43XG43HE4PXG4PPG43AnEeOKsw0uOAG63e9/hqXCct9BAV0q1DRGIibMeCZ0PurgDiA8+mhUIQH01eKsw3irqqiuoqSyjpqqMWq+Puvp6AtWlBGpKMbXleP0QqK/F4S0jxluGq74Cj6+GJH8l7kAxLn89LuPFZepxGy9uvN+7xPRI+YyDGjyU46EeJ36c+HCxa9TtjJ18Y6utp0GLAl1EJgJPYP1ifs4Y8/B+/R7g38CxwB7gUmPM5tYtVSnV4Tkc4EkETyICxKZDLNbs9K3G77P+svB7wVeLqa+l3ltNfV0tvnov9d46/N5afN5a/N5q/N5aAt4aTH3DoxZ/wE89LvDVIfXVOHzV1vv5fRCoJz2jW2tWvM9BA11EnMBTwBlAAbBERGYZY1aHLHYtsNcYM0BEpgCPAJe2RcFKKdWmnC5wJu57KYA7+GjvWjJ03Ggg3xiz0RjjBWYAk/dbZjLwUvD5W8AE0YtclVIqrFoS6D2AbSGvC/jhGZN9yxhjfEAZkL7/G4nIDSKSKyK5xcXFh1exUkqpJoV1cGdjzHRjTI4xJicjIyOcq1ZKqajXkkAvBHqFvO4ZbGtyGRFxASlYJ0eVUkqFSUsCfQkwUET6iogbmALM2m+ZWcBVwec/Bj41dg3jqJRSHdRBr3IxxvhE5BZgLtZliy8YY/JE5H4g1xgzC3geeFlE8oESrNBXSikVRi26Dt0YMweYs1/bvSHPa4GLW7c0pZRSh0JnvFVKqShh24xFIlIMbDnMb+8M7G7FclpTe61N6zo0Wteha6+1RVtdfYwxTV4maFugHwkRyW1uCia7tdfatK5Do3UduvZaW0eqSw+5KKVUlNBAV0qpKBGpgT7d7gIOoL3WpnUdGq3r0LXX2jpMXRF5DF0ppdQPReoeulJKqf1ooCulVJSIuEAXkYkisk5E8kVkqo119BKRz0RktYjkichtwfb7RKRQRJYHH+fYUNtmEfk2uP7cYFsnEflIRNYHv7bqJC8tqGlwyDZZLiLlInK7XdtLRF4QkSIRWRXS1uQ2Esvfg5+5lSIyKsx1PSoia4Pr/q+IpAbbs0SkJmTbPR3mupr92YnIXcHttU5Ezmqrug5Q2xshdW0WkeXB9rBsswPkQ9t+xowxEfPAGktmA9APawKRFUC2TbVkAqOCz5OA74Bs4D7g1zZvp81A5/3a/gJMDT6fCjxi889xJ9DHru0FnAyMAlYdbBsB5wDvY01eczywKMx1nQm4gs8fCakrK3Q5G7ZXkz+74P+DFYAH6Bv8P+sMZ2379T8G3BvObXaAfGjTz1ik7aG3ZPaksDDG7DDGfBN8XgGsobmp0tuH0FmlXgIusK8UJgAbjDGHe6fwETPGfIk1kFyo5rbRZODfxrIQSBWRzHDVZYz50FgTxwAsxBrCOqya2V7NmQzMMMbUGWM2AflY/3fDXpuICHAJ8Hpbrb+ZmprLhzb9jEVaoLdk9qSwE5EsYCSwKNh0S/DPphfCfWgjyAAfishSEbkh2NbVGLMj+Hwn0NWGuhpM4fv/wezeXg2a20bt6XP3M6w9uQZ9RWSZiHwhIifZUE9TP7v2tL1OAnYZY9aHtIV1m+2XD236GYu0QG93RCQR+A9wuzGmHPgn0B8YAezA+nMv3MYZY0YBZwO/EJGTQzuN9TeeLderijWm/iTgzWBTe9heP2DnNmqOiNwN+IBXg007gN7GmJHAHcBrIpIcxpLa5c9uP5fx/Z2HsG6zJvJhn7b4jEVaoLdk9qSwEZEYrB/Wq8aYtwGMMbuMMX5jTAB4ljb8U7M5xpjC4Nci4L/BGnY1/AkX/FoU7rqCzga+McbsCtZo+/YK0dw2sv1zJyJXA+cBVwSDgOAhjT3B50uxjlUPCldNB/jZ2b69YN/saT8C3mhoC+c2ayofaOPPWKQFektmTwqL4LG554E1xpi/hbSHHve6EFi1//e2cV0JIpLU8BzrhNoqvj+r1FXAO+GsK8T39pjs3l77aW4bzQJ+GrwS4XigLOTP5jYnIhOB3wKTjDHVIe0ZIuIMPu8HDAQ2hrGu5n52s4ApIuIRkb7BuhaHq64QpwNrjTEFDQ3h2mbN5QNt/Rlr67O9rf3AOhv8HdZv1rttrGMc1p9LK4Hlwcc5wMvAt8H2WUBmmOvqh3WFwQogr2EbAenAJ8B64GOgkw3bLAFrrtmUkDZbthfWL5UdQD3W8cprm9tGWFcePBX8zH0L5IS5rnys46sNn7Ong8teFPwZLwe+Ac4Pc13N/uyAu4Pbax1wdrh/lsH2F4Gb9ls2LNvsAPnQpp8xvfVfKaWiRKQdclFKKdUMDXSllIoSGuhKKRUlNNCVUipKaKArpVSU0EBXSqkooYGulFJR4v8DwyuhXT0UjeQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d734b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
